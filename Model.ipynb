{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import SelectKBest,f_regression,mutual_info_regression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('X_train.csv',index_col = 0)\n",
    "X_predict= pd.read_csv('X_predict.csv',index_col = 0)\n",
    "y = pd.read_csv('y.csv',header = None,  names=['Id', 'price'],index_col = 0).iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation function and grid search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "\n",
    "def rmsle_cv(model,x_in):\n",
    "    kf = KFold(n_folds, shuffle=True).get_n_splits(x_in)\n",
    "    rmse= np.sqrt(-cross_val_score(model, x_in, y_train.values, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)\n",
    "\n",
    "class grid():\n",
    "    def __init__(self,model):\n",
    "        self.model = model\n",
    "    \n",
    "    def grid_get(self,x_in,param_grid):\n",
    "        kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(x_in)\n",
    "        grid_search = GridSearchCV(self.model,param_grid,cv=kf, scoring=\"neg_mean_squared_error\", n_jobs=-1,verbose=2)\n",
    "        grid_search.fit(x_in, y_train.values)\n",
    "        print(grid_search.best_params_, np.sqrt(-grid_search.best_score_))\n",
    "        grid_search.cv_results_['mean_test_score'] = np.sqrt(-grid_search.cv_results_['mean_test_score'])\n",
    "        print(pd.DataFrame(grid_search.cv_results_)[['params','mean_test_score','std_test_score']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examing num of feature used in the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, y_train, y_test = train_test_split(X_train, y, test_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0006))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.seterr(divide='ignore', invalid='ignore')\n",
    "# num_feats_ = np.arange(100,286,2)\n",
    "# cv_score = []\n",
    "# test_score = []\n",
    "# for num_feats in num_feats_: \n",
    "#     X_new = SelectKBest(f_regression, k= num_feats).fit_transform(train.values, y_train.values)\n",
    "    \n",
    "#     kf = KFold(n_folds, shuffle=True).get_n_splits(X_new)\n",
    "#     score= np.sqrt(-cross_val_score(lasso, X_new, y_train.values, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "#     cv_score.append(score.mean())\n",
    "    \n",
    "\n",
    "# plt.plot(num_feats_,cv_score,'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 580 features to do the training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  35 out of  35 | elapsed:    9.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.0004} 0.11722603418075245\n",
      "              params  mean_test_score  std_test_score\n",
      "0  {'alpha': 0.0001}         0.121113        0.002922\n",
      "1  {'alpha': 0.0002}         0.118798        0.002595\n",
      "2  {'alpha': 0.0003}         0.117754        0.002374\n",
      "3  {'alpha': 0.0004}         0.117226        0.002259\n",
      "4  {'alpha': 0.0005}         0.117259        0.002184\n",
      "5  {'alpha': 0.0006}         0.117444        0.002154\n",
      "6  {'alpha': 0.0007}         0.117666        0.002123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "grid(Lasso()).grid_get(train.values,{'alpha': [0.0001,0.0002,0.0003,0.0004,0.0005,0.0006,0.0007]})\n",
    "\n",
    "# lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.001, random_state=1))\n",
    "# score= rmsle_cv(lasso,train.values)\n",
    "# print(\"Lasso score: {:.5f} ({:.5f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic Net Regression :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid(ElasticNet()).grid_get(X_new,{'alpha': [0.0006,0.0007,0.0008,0.0009],'l1_ratio':[0.8,0.9,1,1.1],'max_iter':[700,800,900,1000,1100]})\n",
    "\n",
    "# ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0004, l1_ratio=1.4,max_iter = 600))\n",
    "\n",
    "# score = rmsle_cv(ENet,train.values)\n",
    "# print(\"ElasticNet score: {:.5f} ({:.5f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Ridge Regression :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# grid(KernelRidge()).grid_get(X_new,[{'kernel': ['linear'],'alpha':np.logspace(-4,4,8)},{'kernel': ['polynomial'],'alpha':np.logspace(-4,4,8),'degree':np.logspace(-4,4,8),'coef0':np.logspace(-4,4,8)}])\n",
    "\n",
    "KRR = KernelRidge(alpha=0.04, kernel='polynomial', degree=1, coef0=0.0008)\n",
    "\n",
    "score = rmsle_cv(KRR,train.values)\n",
    "print(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid(KNeighborsRegressor()).grid_get(X_new,{'n_neighbors':np.arange(5,50,5), 'weights':['uniform','distance'], 'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'], 'leaf_size':np.arange(5,50,5), 'p':np.arange(2,20,2)})\n",
    "\n",
    "\n",
    "KNN = make_pipeline(RobustScaler(), KNeighborsRegressor(n_neighbors=6, weights = 'distance',algorithm='brute', p=1, leaf_size=1))\n",
    "\n",
    "score = rmsle_cv(KNN,train.values)\n",
    "print(\"KNN score: {:.5f} ({:.5f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid(SVR()).grid_get(X_new,{'kernel':['linear', 'poly', 'rbf', 'sigmoid'], 'degree':[1,2,3,4,5], 'coef0':[0.0,0.1,0.2,1,10],'C':[0.01,0.1,1.0,10], 'epsilon':[0.01,0.1,1,10]})\n",
    "\n",
    "#{'coef0': 0.0, 'epsilon': 0.02, 'kernel': 'linear', 'C': 0.004, 'degree': 1} 0.11325162650394656\n",
    "\n",
    "# SVR = make_pipeline(RobustScaler(), SVR(kernel='poly', degree=1, coef0=1,C=9.0, epsilon=0.01))\n",
    "\n",
    "# score = rmsle_cv(SVR,X_new)\n",
    "# print(\"SVR score: {:.5f} ({:.5f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid(RandomForestRegressor()).grid_get(X_new,{'n_estimators':[100,1000,5000], 'max_depth':[None,2,5,10,20], 'min_samples_split':[2,5,10], 'min_samples_leaf':[1,2,3,5,10], 'max_features':[0.01,0.05,0.1,0.5,1]})\n",
    "\n",
    "\n",
    "RF = make_pipeline(RobustScaler(), RandomForestRegressor(n_estimators=500, max_depth=None, min_samples_split=2, min_samples_leaf=1, max_features=0.3))\n",
    "\n",
    "score = rmsle_cv(RF,train.values)\n",
    "print(\"RF score: {:.5f} ({:.5f})\\n\".format(score.mean(), score.std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Regression :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#{'min_samples_leaf': 4, 'max_features': 0.04, 'min_samples_split': 30, 'n_estimators': 3700, 'learning_rate': 0.01, 'max_depth': 3, 'loss': 'ls'} 0.1122755329486732\n",
    "\n",
    "GBoost = GradientBoostingRegressor(n_estimators=3700, learning_rate=0.04,\n",
    "                                   max_depth=3, max_features=0.04,\n",
    "                                   min_samples_leaf=4, min_samples_split=30, \n",
    "                                   loss='ls')\n",
    "score = rmsle_cv(GBoost,train.values)\n",
    "print(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid(xgb.XGBRegressor()).grid_get({'colsample_bytree':[0.1,0.4603], 'gamma':[0.01,0.0468], 'learning_rate':[0.01,0.05], 'max_depth':[3,10], 'min_child_weight':[1.7817,10], 'n_estimators':[2000,5200],'reg_alpha':[0.4640,1], 'reg_lambda':[0.5,0.9],'subsample':[0.1,0.5213]})\n",
    "\n",
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, learning_rate=0.05, max_depth=3, min_child_weight=1.7817, n_estimators=5200,reg_alpha=0.4640, reg_lambda=0.9,subsample=0.5213, silent=1,nthread = -1)\n",
    "\n",
    "score = rmsle_cv(model_xgb,train.values)\n",
    "print(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid(lgb.LGBMRegressor()).grid_get(X_new,{'num_leaves':[3],'learning_rate':[0.05], 'n_estimators':[1000],'max_bin' : [220,250,270], 'bagging_fraction' : [0.8],'bagging_freq' : [12,14,16,18], 'feature_fraction' : [0.1],'min_data_in_leaf' :[4,5,6], 'min_sum_hessian_in_leaf' : [2,4,6,8]})\n",
    "\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=3,learning_rate = 0.05, n_estimators=1000, max_bin = 250, bagging_fraction = 0.8,bagging_freq = 16, feature_fraction = 0.1,bagging_seed=9,min_data_in_leaf =5, min_sum_hessian_in_leaf = 2)\n",
    "\n",
    "\n",
    "score = rmsle_cv(model_lgb,train.values)\n",
    "print(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simplest Stacking approach : Averaging base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    #Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_models = AveragingModels(models = (lasso,ENet, KRR, GBoost,model_xgb, model_lgb))\n",
    "\n",
    "score = rmsle_cv(averaged_models,train.values)\n",
    "print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Less simple Stacking : Adding a Meta-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "   \n",
    "    # We again fit the data on clones of the original models\n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "        # Train cloned base models then create out-of-fold predictions\n",
    "        # that are needed to train the cloned meta-model\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    #Do the predictions of all base models on the test data and use the averaged predictions as \n",
    "    #meta-features for the final prediction which is done by the meta-model\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_averaged_models = StackingAveragedModels(base_models = (lasso, GBoost, KRR),\n",
    "                                                 meta_model = ENet)\n",
    "\n",
    "score = rmsle_cv(stacked_averaged_models,train.values)\n",
    "print(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_averaged_models.fit(train.values, y_train.values)\n",
    "stacked_pred = np.expm1(stacked_averaged_models.predict(X_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb.fit(train.values, y_train.values)\n",
    "\n",
    "model_xgb_pred = np.expm1(model_xgb.predict(X_predict.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgb.fit(train.values, y_train.values)\n",
    "model_lgb_pred = np.expm1(model_lgb.predict(X_predict.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = stacked_pred*0.8 + model_xgb_pred*0.1 + model_lgb_pred*0.1\n",
    "\n",
    "submission_df = pd.DataFrame(data= {'Id' :X_predict.index, 'SalePrice': ensemble})\n",
    "\n",
    "submission_df.head()\n",
    "\n",
    "submission_df.to_csv('submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
